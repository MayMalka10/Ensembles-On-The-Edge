{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjTBdENUSPGZ",
        "outputId": "f4b9ad1e-dc0d-41d7-afbd-1d099563e323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /tmp/cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 42083852.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/cifar100/cifar-100-python.tar.gz to /tmp/cifar100\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 164MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  1  batch: 100 [  6400/50000]  total loss: 3.09962845                  time = [0.7358582059542338] minutes\n",
            "epoch:  1  batch: 200 [ 12800/50000]  total loss: 2.60135627                  time = [1.356647789478302] minutes\n",
            "epoch:  1  batch: 300 [ 19200/50000]  total loss: 2.29938149                  time = [1.9890548467636109] minutes\n",
            "epoch:  1  batch: 400 [ 25600/50000]  total loss: 2.33984923                  time = [2.631840443611145] minutes\n",
            "epoch:  1  batch: 500 [ 32000/50000]  total loss: 2.30282784                  time = [3.2686692476272583] minutes\n",
            "epoch:  1  batch: 600 [ 38400/50000]  total loss: 2.28998470                  time = [3.908490558465322] minutes\n",
            "epoch:  1  batch: 700 [ 44800/50000]  total loss: 1.71812284                  time = [4.5471675554911295] minutes\n",
            "Train Ensemble Accuracy at epoch 1 is 39.33599853515625%\n",
            "Validation Ensemble Accuracy at epoch 1 is 50.939998626708984%\n",
            "epoch:  2  batch: 100 [  6400/50000]  total loss: 2.02938581                  time = [5.994772811730702] minutes\n",
            "epoch:  2  batch: 200 [ 12800/50000]  total loss: 2.02803421                  time = [6.6349894285202025] minutes\n",
            "epoch:  2  batch: 300 [ 19200/50000]  total loss: 2.04192400                  time = [7.273633933067321] minutes\n",
            "epoch:  2  batch: 400 [ 25600/50000]  total loss: 2.04087448                  time = [7.912321917215983] minutes\n",
            "epoch:  2  batch: 500 [ 32000/50000]  total loss: 1.72851634                  time = [8.551177525520325] minutes\n",
            "epoch:  2  batch: 600 [ 38400/50000]  total loss: 1.56252468                  time = [9.189805567264557] minutes\n",
            "epoch:  2  batch: 700 [ 44800/50000]  total loss: 1.70996892                  time = [9.829656453927358] minutes\n",
            "Train Ensemble Accuracy at epoch 2 is 54.715999603271484%\n",
            "Validation Ensemble Accuracy at epoch 2 is 58.65999984741211%\n",
            "epoch:  3  batch: 100 [  6400/50000]  total loss: 1.70941687                  time = [11.280427742004395] minutes\n",
            "epoch:  3  batch: 200 [ 12800/50000]  total loss: 1.66649866                  time = [11.919434022903442] minutes\n",
            "epoch:  3  batch: 300 [ 19200/50000]  total loss: 1.54462433                  time = [12.55980031490326] minutes\n",
            "epoch:  3  batch: 400 [ 25600/50000]  total loss: 1.43673289                  time = [13.198711184660594] minutes\n",
            "epoch:  3  batch: 500 [ 32000/50000]  total loss: 1.76193810                  time = [13.83710455497106] minutes\n",
            "epoch:  3  batch: 600 [ 38400/50000]  total loss: 1.84976363                  time = [14.475826597213745] minutes\n",
            "epoch:  3  batch: 700 [ 44800/50000]  total loss: 1.28389597                  time = [15.115131274859111] minutes\n",
            "Train Ensemble Accuracy at epoch 3 is 59.95800018310547%\n",
            "Validation Ensemble Accuracy at epoch 3 is 61.90999984741211%\n",
            "epoch:  4  batch: 100 [  6400/50000]  total loss: 1.56176186                  time = [16.563751566410065] minutes\n",
            "epoch:  4  batch: 200 [ 12800/50000]  total loss: 1.21945512                  time = [17.20117042462031] minutes\n",
            "epoch:  4  batch: 300 [ 19200/50000]  total loss: 1.41756916                  time = [17.840029211839042] minutes\n",
            "epoch:  4  batch: 400 [ 25600/50000]  total loss: 1.29210770                  time = [18.478356798489887] minutes\n",
            "epoch:  4  batch: 500 [ 32000/50000]  total loss: 1.37995660                  time = [19.117208433151244] minutes\n",
            "epoch:  4  batch: 600 [ 38400/50000]  total loss: 1.38247359                  time = [19.757164951165517] minutes\n",
            "epoch:  4  batch: 700 [ 44800/50000]  total loss: 1.54819715                  time = [20.396491209665935] minutes\n",
            "Train Ensemble Accuracy at epoch 4 is 63.913997650146484%\n",
            "Validation Ensemble Accuracy at epoch 4 is 63.34000015258789%\n",
            "epoch:  5  batch: 100 [  6400/50000]  total loss: 1.56077242                  time = [21.847741198539733] minutes\n",
            "epoch:  5  batch: 200 [ 12800/50000]  total loss: 1.75594175                  time = [22.48729599316915] minutes\n",
            "epoch:  5  batch: 300 [ 19200/50000]  total loss: 1.37598467                  time = [23.125700736045836] minutes\n",
            "epoch:  5  batch: 400 [ 25600/50000]  total loss: 1.34476864                  time = [23.763752170403798] minutes\n",
            "epoch:  5  batch: 500 [ 32000/50000]  total loss: 1.28571486                  time = [24.4021453221639] minutes\n",
            "epoch:  5  batch: 600 [ 38400/50000]  total loss: 1.17056000                  time = [25.04067347844442] minutes\n",
            "epoch:  5  batch: 700 [ 44800/50000]  total loss: 1.27987063                  time = [25.679543975989024] minutes\n",
            "Train Ensemble Accuracy at epoch 5 is 66.72000122070312%\n",
            "Validation Ensemble Accuracy at epoch 5 is 65.81999969482422%\n",
            "epoch:  6  batch: 100 [  6400/50000]  total loss: 1.34704196                  time = [27.129118863741557] minutes\n",
            "epoch:  6  batch: 200 [ 12800/50000]  total loss: 1.05269015                  time = [27.767723083496094] minutes\n",
            "epoch:  6  batch: 300 [ 19200/50000]  total loss: 1.19789994                  time = [28.40536554257075] minutes\n",
            "epoch:  6  batch: 400 [ 25600/50000]  total loss: 1.12899184                  time = [29.0431356549263] minutes\n",
            "epoch:  6  batch: 500 [ 32000/50000]  total loss: 1.31272352                  time = [29.680221223831175] minutes\n",
            "epoch:  6  batch: 600 [ 38400/50000]  total loss: 1.63635838                  time = [30.3170503338178] minutes\n",
            "epoch:  6  batch: 700 [ 44800/50000]  total loss: 1.16060209                  time = [30.953580927848815] minutes\n",
            "Train Ensemble Accuracy at epoch 6 is 68.93000030517578%\n",
            "Validation Ensemble Accuracy at epoch 6 is 66.93999481201172%\n",
            "epoch:  7  batch: 100 [  6400/50000]  total loss: 1.22283947                  time = [32.39696614344915] minutes\n",
            "epoch:  7  batch: 200 [ 12800/50000]  total loss: 1.29137790                  time = [33.03397970199585] minutes\n",
            "epoch:  7  batch: 300 [ 19200/50000]  total loss: 1.61071265                  time = [33.67116134564082] minutes\n",
            "epoch:  7  batch: 400 [ 25600/50000]  total loss: 1.38953817                  time = [34.308008499940236] minutes\n",
            "epoch:  7  batch: 500 [ 32000/50000]  total loss: 1.36863375                  time = [34.94496552546819] minutes\n",
            "epoch:  7  batch: 600 [ 38400/50000]  total loss: 1.28511608                  time = [35.581753464539844] minutes\n",
            "epoch:  7  batch: 700 [ 44800/50000]  total loss: 1.20930433                  time = [36.21857144435247] minutes\n",
            "Train Ensemble Accuracy at epoch 7 is 70.98799896240234%\n",
            "Validation Ensemble Accuracy at epoch 7 is 68.13999938964844%\n",
            "epoch:  8  batch: 100 [  6400/50000]  total loss: 1.16584182                  time = [37.66275652647018] minutes\n",
            "epoch:  8  batch: 200 [ 12800/50000]  total loss: 1.04336298                  time = [38.29985982577006] minutes\n",
            "epoch:  8  batch: 300 [ 19200/50000]  total loss: 1.52108383                  time = [38.93746038675308] minutes\n",
            "epoch:  8  batch: 400 [ 25600/50000]  total loss: 1.38184750                  time = [39.57461777528127] minutes\n",
            "epoch:  8  batch: 500 [ 32000/50000]  total loss: 1.24412358                  time = [40.211907994747165] minutes\n",
            "epoch:  8  batch: 600 [ 38400/50000]  total loss: 1.12522888                  time = [40.850797986984254] minutes\n",
            "epoch:  8  batch: 700 [ 44800/50000]  total loss: 1.21026766                  time = [41.48929706811905] minutes\n",
            "Train Ensemble Accuracy at epoch 8 is 72.8699951171875%\n",
            "Validation Ensemble Accuracy at epoch 8 is 68.6500015258789%\n",
            "epoch:  9  batch: 100 [  6400/50000]  total loss: 1.26744807                  time = [42.93896217346192] minutes\n",
            "epoch:  9  batch: 200 [ 12800/50000]  total loss: 1.00161493                  time = [43.57751005093257] minutes\n",
            "epoch:  9  batch: 300 [ 19200/50000]  total loss: 0.99932897                  time = [44.214229142665864] minutes\n",
            "epoch:  9  batch: 400 [ 25600/50000]  total loss: 1.21270621                  time = [44.8505697965622] minutes\n",
            "epoch:  9  batch: 500 [ 32000/50000]  total loss: 1.34969950                  time = [45.487431315581006] minutes\n",
            "epoch:  9  batch: 600 [ 38400/50000]  total loss: 1.35962045                  time = [46.125322405497236] minutes\n",
            "epoch:  9  batch: 700 [ 44800/50000]  total loss: 1.22005010                  time = [46.76195485194524] minutes\n",
            "Train Ensemble Accuracy at epoch 9 is 74.26399993896484%\n",
            "Validation Ensemble Accuracy at epoch 9 is 70.38999938964844%\n",
            "epoch: 10  batch: 100 [  6400/50000]  total loss: 1.03212309                  time = [48.209343659877774] minutes\n",
            "epoch: 10  batch: 200 [ 12800/50000]  total loss: 0.94033313                  time = [48.846817096074425] minutes\n",
            "epoch: 10  batch: 300 [ 19200/50000]  total loss: 1.06373167                  time = [49.48492521047592] minutes\n",
            "epoch: 10  batch: 400 [ 25600/50000]  total loss: 1.10619438                  time = [50.12272381782532] minutes\n",
            "epoch: 10  batch: 500 [ 32000/50000]  total loss: 1.08784151                  time = [50.759960023562115] minutes\n",
            "epoch: 10  batch: 600 [ 38400/50000]  total loss: 1.20981741                  time = [51.39685497283936] minutes\n",
            "epoch: 10  batch: 700 [ 44800/50000]  total loss: 1.08560395                  time = [52.03404455979665] minutes\n",
            "Train Ensemble Accuracy at epoch 10 is 75.59400177001953%\n",
            "Validation Ensemble Accuracy at epoch 10 is 69.72999572753906%\n",
            "epoch: 11  batch: 100 [  6400/50000]  total loss: 1.07976401                  time = [53.479976284503934] minutes\n",
            "epoch: 11  batch: 200 [ 12800/50000]  total loss: 1.16597986                  time = [54.116872262954715] minutes\n",
            "epoch: 11  batch: 300 [ 19200/50000]  total loss: 1.12625480                  time = [54.753656009833016] minutes\n",
            "epoch: 11  batch: 400 [ 25600/50000]  total loss: 1.35726881                  time = [55.39031997919083] minutes\n",
            "epoch: 11  batch: 500 [ 32000/50000]  total loss: 1.22049415                  time = [56.027148640155794] minutes\n",
            "epoch: 11  batch: 600 [ 38400/50000]  total loss: 0.85490268                  time = [56.6637064854304] minutes\n",
            "epoch: 11  batch: 700 [ 44800/50000]  total loss: 1.14024949                  time = [57.30013993183772] minutes\n",
            "Train Ensemble Accuracy at epoch 11 is 77.06199645996094%\n",
            "Validation Ensemble Accuracy at epoch 11 is 70.25999450683594%\n",
            "epoch: 12  batch: 100 [  6400/50000]  total loss: 0.76187414                  time = [58.74423917929332] minutes\n",
            "epoch: 12  batch: 200 [ 12800/50000]  total loss: 0.98144823                  time = [59.37982008854548] minutes\n",
            "epoch: 12  batch: 300 [ 19200/50000]  total loss: 1.10746145                  time = [60.01621287663777] minutes\n",
            "epoch: 12  batch: 400 [ 25600/50000]  total loss: 1.37604034                  time = [60.65254676739375] minutes\n",
            "epoch: 12  batch: 500 [ 32000/50000]  total loss: 1.21522284                  time = [61.28900148073832] minutes\n",
            "epoch: 12  batch: 600 [ 38400/50000]  total loss: 1.00804627                  time = [61.92575025955836] minutes\n",
            "epoch: 12  batch: 700 [ 44800/50000]  total loss: 0.84948367                  time = [62.56239092350006] minutes\n",
            "Train Ensemble Accuracy at epoch 12 is 78.22200012207031%\n",
            "Validation Ensemble Accuracy at epoch 12 is 71.8699951171875%\n",
            "epoch: 13  batch: 100 [  6400/50000]  total loss: 1.03768194                  time = [64.00544414520263] minutes\n",
            "epoch: 13  batch: 200 [ 12800/50000]  total loss: 1.03635919                  time = [64.64001927375793] minutes\n",
            "epoch: 13  batch: 300 [ 19200/50000]  total loss: 0.99530804                  time = [65.27459026177725] minutes\n",
            "epoch: 13  batch: 400 [ 25600/50000]  total loss: 1.18253648                  time = [65.90919015804927] minutes\n",
            "epoch: 13  batch: 500 [ 32000/50000]  total loss: 1.21171665                  time = [66.54402107000351] minutes\n",
            "epoch: 13  batch: 600 [ 38400/50000]  total loss: 1.07800281                  time = [67.17870169480642] minutes\n",
            "epoch: 13  batch: 700 [ 44800/50000]  total loss: 0.82688224                  time = [67.81327556371689] minutes\n",
            "Train Ensemble Accuracy at epoch 13 is 79.76799774169922%\n",
            "Validation Ensemble Accuracy at epoch 13 is 72.11000061035156%\n",
            "epoch: 14  batch: 100 [  6400/50000]  total loss: 0.86237168                  time = [69.25462741454443] minutes\n",
            "epoch: 14  batch: 200 [ 12800/50000]  total loss: 0.88939863                  time = [69.88958315849304] minutes\n",
            "epoch: 14  batch: 300 [ 19200/50000]  total loss: 0.89927614                  time = [70.52438469727834] minutes\n",
            "epoch: 14  batch: 400 [ 25600/50000]  total loss: 0.90450466                  time = [71.15941645701726] minutes\n",
            "epoch: 14  batch: 500 [ 32000/50000]  total loss: 0.94767499                  time = [71.79499369462332] minutes\n",
            "epoch: 14  batch: 600 [ 38400/50000]  total loss: 1.12667203                  time = [72.4300618648529] minutes\n",
            "epoch: 14  batch: 700 [ 44800/50000]  total loss: 1.01143670                  time = [73.06515148083369] minutes\n",
            "Train Ensemble Accuracy at epoch 14 is 80.71199798583984%\n",
            "Validation Ensemble Accuracy at epoch 14 is 72.50999450683594%\n",
            "epoch: 15  batch: 100 [  6400/50000]  total loss: 0.90793836                  time = [74.51155259211858] minutes\n",
            "epoch: 15  batch: 200 [ 12800/50000]  total loss: 0.87110913                  time = [75.14769159952799] minutes\n",
            "epoch: 15  batch: 300 [ 19200/50000]  total loss: 0.72033316                  time = [75.78562778631846] minutes\n",
            "epoch: 15  batch: 400 [ 25600/50000]  total loss: 0.90288824                  time = [76.42216424942016] minutes\n",
            "epoch: 15  batch: 500 [ 32000/50000]  total loss: 0.91705227                  time = [77.05757595698039] minutes\n",
            "epoch: 15  batch: 600 [ 38400/50000]  total loss: 0.79997373                  time = [77.69400852918625] minutes\n",
            "epoch: 15  batch: 700 [ 44800/50000]  total loss: 1.18270624                  time = [78.33143130540847] minutes\n",
            "Train Ensemble Accuracy at epoch 15 is 81.65999603271484%\n",
            "Validation Ensemble Accuracy at epoch 15 is 73.18999481201172%\n",
            "epoch: 16  batch: 100 [  6400/50000]  total loss: 0.79942423                  time = [79.77641623417536] minutes\n",
            "epoch: 16  batch: 200 [ 12800/50000]  total loss: 0.84113854                  time = [80.41120756864548] minutes\n",
            "epoch: 16  batch: 300 [ 19200/50000]  total loss: 0.60095459                  time = [81.04617240826289] minutes\n",
            "epoch: 16  batch: 400 [ 25600/50000]  total loss: 1.18280900                  time = [81.68203741312027] minutes\n",
            "epoch: 16  batch: 500 [ 32000/50000]  total loss: 0.92255157                  time = [82.31694103479386] minutes\n",
            "epoch: 16  batch: 600 [ 38400/50000]  total loss: 0.96527243                  time = [82.95218909184138] minutes\n",
            "epoch: 16  batch: 700 [ 44800/50000]  total loss: 1.12060308                  time = [83.58805835644404] minutes\n",
            "Train Ensemble Accuracy at epoch 16 is 82.91799926757812%\n",
            "Validation Ensemble Accuracy at epoch 16 is 73.0%\n",
            "epoch: 17  batch: 100 [  6400/50000]  total loss: 0.84920228                  time = [85.03365684747696] minutes\n",
            "epoch: 17  batch: 200 [ 12800/50000]  total loss: 0.72405386                  time = [85.66872278849284] minutes\n",
            "epoch: 17  batch: 300 [ 19200/50000]  total loss: 0.73271221                  time = [86.3031113465627] minutes\n",
            "epoch: 17  batch: 400 [ 25600/50000]  total loss: 0.93805522                  time = [86.93748619953791] minutes\n",
            "epoch: 17  batch: 500 [ 32000/50000]  total loss: 0.85777670                  time = [87.57262378931046] minutes\n",
            "epoch: 17  batch: 600 [ 38400/50000]  total loss: 0.86365813                  time = [88.20838083823521] minutes\n",
            "epoch: 17  batch: 700 [ 44800/50000]  total loss: 0.99729633                  time = [88.84487221638362] minutes\n",
            "Train Ensemble Accuracy at epoch 17 is 83.84600067138672%\n",
            "Validation Ensemble Accuracy at epoch 17 is 72.22999572753906%\n",
            "epoch: 18  batch: 100 [  6400/50000]  total loss: 0.59257120                  time = [90.28919894695282] minutes\n",
            "epoch: 18  batch: 200 [ 12800/50000]  total loss: 0.81490451                  time = [90.92374544938406] minutes\n",
            "epoch: 18  batch: 300 [ 19200/50000]  total loss: 0.98366833                  time = [91.55801968971888] minutes\n",
            "epoch: 18  batch: 400 [ 25600/50000]  total loss: 0.84982949                  time = [92.19219983021418] minutes\n",
            "epoch: 18  batch: 500 [ 32000/50000]  total loss: 0.85559899                  time = [92.82719337145487] minutes\n",
            "epoch: 18  batch: 600 [ 38400/50000]  total loss: 0.84118021                  time = [93.46288223266602] minutes\n",
            "epoch: 18  batch: 700 [ 44800/50000]  total loss: 0.71108055                  time = [94.09853307406108] minutes\n",
            "Train Ensemble Accuracy at epoch 18 is 84.72000122070312%\n",
            "Validation Ensemble Accuracy at epoch 18 is 74.05999755859375%\n",
            "epoch: 19  batch: 100 [  6400/50000]  total loss: 0.65685189                  time = [95.54067025581996] minutes\n",
            "epoch: 19  batch: 200 [ 12800/50000]  total loss: 0.68754590                  time = [96.17665961186091] minutes\n",
            "epoch: 19  batch: 300 [ 19200/50000]  total loss: 0.85944510                  time = [96.81134030421575] minutes\n",
            "epoch: 19  batch: 400 [ 25600/50000]  total loss: 0.79584366                  time = [97.44590819279352] minutes\n",
            "epoch: 19  batch: 500 [ 32000/50000]  total loss: 0.90520936                  time = [98.08049768606821] minutes\n",
            "epoch: 19  batch: 600 [ 38400/50000]  total loss: 0.65483063                  time = [98.71717591285706] minutes\n",
            "epoch: 19  batch: 700 [ 44800/50000]  total loss: 0.86698884                  time = [99.35311518112819] minutes\n",
            "Train Ensemble Accuracy at epoch 19 is 85.36399841308594%\n",
            "Validation Ensemble Accuracy at epoch 19 is 74.5%\n",
            "epoch: 20  batch: 100 [  6400/50000]  total loss: 0.65083277                  time = [100.79751424789428] minutes\n",
            "epoch: 20  batch: 200 [ 12800/50000]  total loss: 0.62595296                  time = [101.43160022099813] minutes\n",
            "epoch: 20  batch: 300 [ 19200/50000]  total loss: 0.78116298                  time = [102.0656365553538] minutes\n",
            "epoch: 20  batch: 400 [ 25600/50000]  total loss: 0.54208308                  time = [102.70025720198949] minutes\n",
            "epoch: 20  batch: 500 [ 32000/50000]  total loss: 0.50028741                  time = [103.33504211505254] minutes\n",
            "epoch: 20  batch: 600 [ 38400/50000]  total loss: 0.80587095                  time = [103.97027242581049] minutes\n",
            "epoch: 20  batch: 700 [ 44800/50000]  total loss: 0.74444413                  time = [104.60557018518448] minutes\n",
            "Train Ensemble Accuracy at epoch 20 is 86.30400085449219%\n",
            "Validation Ensemble Accuracy at epoch 20 is 74.29999542236328%\n",
            "epoch: 21  batch: 100 [  6400/50000]  total loss: 0.70843554                  time = [106.04893726905188] minutes\n",
            "epoch: 21  batch: 200 [ 12800/50000]  total loss: 0.79096067                  time = [106.68264311154684] minutes\n",
            "epoch: 21  batch: 300 [ 19200/50000]  total loss: 0.78194177                  time = [107.31635622580846] minutes\n",
            "epoch: 21  batch: 400 [ 25600/50000]  total loss: 0.73085153                  time = [107.95006373723348] minutes\n",
            "epoch: 21  batch: 500 [ 32000/50000]  total loss: 0.82142997                  time = [108.58408927520117] minutes\n",
            "epoch: 21  batch: 600 [ 38400/50000]  total loss: 0.77948987                  time = [109.21787981589635] minutes\n",
            "epoch: 21  batch: 700 [ 44800/50000]  total loss: 0.86867940                  time = [109.85180006424586] minutes\n",
            "Train Ensemble Accuracy at epoch 21 is 86.97000122070312%\n",
            "Validation Ensemble Accuracy at epoch 21 is 74.37999725341797%\n",
            "epoch: 22  batch: 100 [  6400/50000]  total loss: 0.55356956                  time = [111.29336756070455] minutes\n",
            "epoch: 22  batch: 200 [ 12800/50000]  total loss: 0.80370009                  time = [111.92708305120468] minutes\n",
            "epoch: 22  batch: 300 [ 19200/50000]  total loss: 0.76420552                  time = [112.56072240670522] minutes\n",
            "epoch: 22  batch: 400 [ 25600/50000]  total loss: 0.90487230                  time = [113.19462388753891] minutes\n",
            "epoch: 22  batch: 500 [ 32000/50000]  total loss: 0.76887023                  time = [113.82973479827245] minutes\n",
            "epoch: 22  batch: 600 [ 38400/50000]  total loss: 0.74278796                  time = [114.4641144434611] minutes\n",
            "epoch: 22  batch: 700 [ 44800/50000]  total loss: 0.83109307                  time = [115.09853965838751] minutes\n",
            "Train Ensemble Accuracy at epoch 22 is 87.72799682617188%\n",
            "Validation Ensemble Accuracy at epoch 22 is 73.8699951171875%\n",
            "epoch: 23  batch: 100 [  6400/50000]  total loss: 0.70704585                  time = [116.53838154474894] minutes\n",
            "epoch: 23  batch: 200 [ 12800/50000]  total loss: 0.62643826                  time = [117.17237632671991] minutes\n",
            "epoch: 23  batch: 300 [ 19200/50000]  total loss: 0.58802027                  time = [117.80586856206259] minutes\n",
            "epoch: 23  batch: 400 [ 25600/50000]  total loss: 0.72621095                  time = [118.43954451084137] minutes\n",
            "epoch: 23  batch: 500 [ 32000/50000]  total loss: 0.92481720                  time = [119.07307051022848] minutes\n",
            "epoch: 23  batch: 600 [ 38400/50000]  total loss: 0.58340460                  time = [119.7072028319041] minutes\n",
            "epoch: 23  batch: 700 [ 44800/50000]  total loss: 0.65615654                  time = [120.34036955833434] minutes\n",
            "Train Ensemble Accuracy at epoch 23 is 88.54399871826172%\n",
            "Validation Ensemble Accuracy at epoch 23 is 74.50999450683594%\n",
            "epoch: 24  batch: 100 [  6400/50000]  total loss: 0.73313427                  time = [121.7774031718572] minutes\n",
            "epoch: 24  batch: 200 [ 12800/50000]  total loss: 0.51232868                  time = [122.41103663047154] minutes\n",
            "epoch: 24  batch: 300 [ 19200/50000]  total loss: 0.57288134                  time = [123.0442603111267] minutes\n",
            "epoch: 24  batch: 400 [ 25600/50000]  total loss: 0.59706575                  time = [123.67773965597152] minutes\n",
            "epoch: 24  batch: 500 [ 32000/50000]  total loss: 0.73193771                  time = [124.31180374622345] minutes\n",
            "epoch: 24  batch: 600 [ 38400/50000]  total loss: 0.60108197                  time = [124.9451717933019] minutes\n",
            "epoch: 24  batch: 700 [ 44800/50000]  total loss: 0.84771228                  time = [125.57919219732284] minutes\n",
            "Train Ensemble Accuracy at epoch 24 is 89.40599822998047%\n",
            "Validation Ensemble Accuracy at epoch 24 is 74.87999725341797%\n",
            "epoch: 25  batch: 100 [  6400/50000]  total loss: 0.59487683                  time = [127.02079535722733] minutes\n",
            "epoch: 25  batch: 200 [ 12800/50000]  total loss: 0.55941123                  time = [127.65509603420894] minutes\n",
            "epoch: 25  batch: 300 [ 19200/50000]  total loss: 0.56318271                  time = [128.2893317937851] minutes\n",
            "epoch: 25  batch: 400 [ 25600/50000]  total loss: 0.50353301                  time = [128.92328145106634] minutes\n",
            "epoch: 25  batch: 500 [ 32000/50000]  total loss: 0.71423239                  time = [129.5570163011551] minutes\n",
            "epoch: 25  batch: 600 [ 38400/50000]  total loss: 0.60469967                  time = [130.19056472380956] minutes\n",
            "epoch: 25  batch: 700 [ 44800/50000]  total loss: 0.64693040                  time = [130.8233995596568] minutes\n",
            "Train Ensemble Accuracy at epoch 25 is 89.8699951171875%\n",
            "Validation Ensemble Accuracy at epoch 25 is 74.91999816894531%\n",
            "epoch: 26  batch: 100 [  6400/50000]  total loss: 0.59874159                  time = [132.26111093759536] minutes\n",
            "epoch: 26  batch: 200 [ 12800/50000]  total loss: 0.59418923                  time = [132.89610441923142] minutes\n",
            "epoch: 26  batch: 300 [ 19200/50000]  total loss: 0.90684879                  time = [133.5308923959732] minutes\n",
            "epoch: 26  batch: 400 [ 25600/50000]  total loss: 0.73526555                  time = [134.1646669824918] minutes\n",
            "epoch: 26  batch: 500 [ 32000/50000]  total loss: 0.53844333                  time = [134.7985474030177] minutes\n",
            "epoch: 26  batch: 600 [ 38400/50000]  total loss: 0.66165400                  time = [135.43253773053488] minutes\n",
            "epoch: 26  batch: 700 [ 44800/50000]  total loss: 0.62351042                  time = [136.06721909840903] minutes\n",
            "Train Ensemble Accuracy at epoch 26 is 90.2959976196289%\n",
            "Validation Ensemble Accuracy at epoch 26 is 74.93999481201172%\n",
            "epoch: 27  batch: 100 [  6400/50000]  total loss: 0.52982634                  time = [137.5076404730479] minutes\n",
            "epoch: 27  batch: 200 [ 12800/50000]  total loss: 0.60647619                  time = [138.14025227626163] minutes\n",
            "epoch: 27  batch: 300 [ 19200/50000]  total loss: 0.78878033                  time = [138.77278522253036] minutes\n",
            "epoch: 27  batch: 400 [ 25600/50000]  total loss: 0.51410478                  time = [139.40521402359008] minutes\n",
            "epoch: 27  batch: 500 [ 32000/50000]  total loss: 0.58761680                  time = [140.03797382513682] minutes\n",
            "epoch: 27  batch: 600 [ 38400/50000]  total loss: 0.71801102                  time = [140.67126477162043] minutes\n",
            "epoch: 27  batch: 700 [ 44800/50000]  total loss: 0.88188809                  time = [141.30481578509014] minutes\n",
            "Train Ensemble Accuracy at epoch 27 is 91.17399597167969%\n",
            "Validation Ensemble Accuracy at epoch 27 is 75.70999908447266%\n",
            "epoch: 28  batch: 100 [  6400/50000]  total loss: 0.54151517                  time = [142.74618970950445] minutes\n",
            "epoch: 28  batch: 200 [ 12800/50000]  total loss: 0.57785124                  time = [143.379552145799] minutes\n",
            "epoch: 28  batch: 300 [ 19200/50000]  total loss: 0.61880499                  time = [144.01293715635936] minutes\n",
            "epoch: 28  batch: 400 [ 25600/50000]  total loss: 0.51693445                  time = [144.64632523059845] minutes\n",
            "epoch: 28  batch: 500 [ 32000/50000]  total loss: 0.51344335                  time = [145.27958741585414] minutes\n",
            "epoch: 28  batch: 600 [ 38400/50000]  total loss: 0.58822358                  time = [145.91331824064255] minutes\n",
            "epoch: 28  batch: 700 [ 44800/50000]  total loss: 0.60333908                  time = [146.5467663606008] minutes\n",
            "Train Ensemble Accuracy at epoch 28 is 91.52799987792969%\n",
            "Validation Ensemble Accuracy at epoch 28 is 75.54000091552734%\n",
            "epoch: 29  batch: 100 [  6400/50000]  total loss: 0.67883545                  time = [147.98842063744863] minutes\n",
            "epoch: 29  batch: 200 [ 12800/50000]  total loss: 0.62920779                  time = [148.6223573923111] minutes\n",
            "epoch: 29  batch: 300 [ 19200/50000]  total loss: 0.72057080                  time = [149.2565539677938] minutes\n",
            "epoch: 29  batch: 400 [ 25600/50000]  total loss: 0.69420815                  time = [149.8905404686928] minutes\n",
            "epoch: 29  batch: 500 [ 32000/50000]  total loss: 0.75489867                  time = [150.52409807443618] minutes\n",
            "epoch: 29  batch: 600 [ 38400/50000]  total loss: 0.63132119                  time = [151.15749345620472] minutes\n",
            "epoch: 29  batch: 700 [ 44800/50000]  total loss: 0.64189363                  time = [151.79104983409246] minutes\n",
            "Train Ensemble Accuracy at epoch 29 is 92.06199645996094%\n",
            "Validation Ensemble Accuracy at epoch 29 is 75.50999450683594%\n",
            "epoch: 30  batch: 100 [  6400/50000]  total loss: 0.44920045                  time = [153.2315088589986] minutes\n",
            "epoch: 30  batch: 200 [ 12800/50000]  total loss: 0.82655430                  time = [153.86447165409723] minutes\n",
            "epoch: 30  batch: 300 [ 19200/50000]  total loss: 0.46892259                  time = [154.49807714621227] minutes\n",
            "epoch: 30  batch: 400 [ 25600/50000]  total loss: 0.47351518                  time = [155.13242954413096] minutes\n",
            "epoch: 30  batch: 500 [ 32000/50000]  total loss: 0.53795898                  time = [155.7653739452362] minutes\n",
            "epoch: 30  batch: 600 [ 38400/50000]  total loss: 0.52388257                  time = [156.39823154211044] minutes\n",
            "epoch: 30  batch: 700 [ 44800/50000]  total loss: 0.61875415                  time = [157.0309684753418] minutes\n",
            "Train Ensemble Accuracy at epoch 30 is 92.72200012207031%\n",
            "Validation Ensemble Accuracy at epoch 30 is 75.58999633789062%\n",
            "Training took: 2.6305908382601206 hours\n",
            "Number of parameters for 9 users is : 18547680\n",
            "Accuracy of 9 users is : 75.70999908447266\n",
            "[18547680]\n",
            "[tensor(75.7100, device='cuda:0')]\n"
          ]
        }
      ],
      "source": [
        "#####################################################################\n",
        "############################## imports ##############################\n",
        "#####################################################################\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import ssl\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#####################################################################\n",
        "########################## HyperParameters ##########################\n",
        "#####################################################################\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 30\n",
        "ARCH_WIDTH = 1\n",
        "NUM_CLASSES = 100 # Classification of CIFAR 100\n",
        "PART_IDX = 1\n",
        "NUM_ENSEMBLE = 16\n",
        "NUM_EMBED = [32,1024] #Number of vectors in the codebook.\n",
        "NUM_PARTS = 1 # Split the vectors in the codebook into ### parts.\n",
        "COMMITMENT_W = 0.1 # Weight to the VQ_LOSS\n",
        "\n",
        "#####################################################################\n",
        "############################## DATASET ##############################\n",
        "#####################################################################\n",
        "\n",
        "def get_test_transforms():\n",
        "    test_transform = transforms.Compose(\n",
        "                [transforms.ToTensor(),\n",
        "      \t\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "    return test_transform\n",
        "\n",
        "def get_train_transforms():\n",
        "    transform = transforms.Compose(\n",
        "                [transforms.RandomCrop(32, padding=4),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 transforms.ToTensor(),\n",
        "                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "    return transform\n",
        "\n",
        "train_transform = get_train_transforms()\n",
        "test_transform = get_test_transforms()\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "path = \"/tmp/cifar100\"\n",
        "trainset = datasets.CIFAR100(root = path, train=True, download=True, transform=train_transform)\n",
        "testset = datasets.CIFAR100(root = path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "#####################################################################\n",
        "########################### ARCHITECTURE ############################\n",
        "#####################################################################\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view((input.shape[0], -1))\n",
        "\n",
        "\n",
        "\n",
        "def weight_noise(m):\n",
        "    ## Reset all the parameters of the new 'Decoder'.\n",
        "    ## For creating an ensembles of decoders.\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.weight.data = m.weight.data + torch.randn((m.weight).shape)*0.05\n",
        "        if m.bias is not None:\n",
        "            m.bias.data = m.bias.data + torch.randn((m.bias.shape))*0.02\n",
        "\n",
        "\n",
        "def SplitNet(width=1, pretrained=True, num_classes=1000, stop_layer=4,decoder_copies=1):\n",
        "    if width != 1:\n",
        "        pretrained = False\n",
        "\n",
        "    encoder_layers = []\n",
        "    decoder_layers = []\n",
        "    EncDec_dict = dict(encoder=[], decoders=[])\n",
        "    inverted_residual_setting=[[1, 16, 1, 1],[6, 24, 2, 1],[6, 32, 3, 1],[6, 64, 4, 2],[6, 96, 3, 1],[6, 160, 3, 1],[6, 320, 1, 1]]\n",
        "    num_channels_per_layer = [32, 16, 24, 24, 32, 32, 32, 64, 64, 64, 64, 96, 96, 96, 160, 160, 160, 320, 1280]\n",
        "\n",
        "    mobilenetv2 = models.mobilenet_v2(pretrained=pretrained, num_classes=1000,width_mult=width,inverted_residual_setting=inverted_residual_setting)\n",
        "    res_stop = 5\n",
        "    for layer_idx, l in enumerate(mobilenetv2.features):\n",
        "        if layer_idx <= res_stop:\n",
        "            encoder_layers.append(l)\n",
        "        else:\n",
        "            decoder_layers.append(l)\n",
        "\n",
        "    dropout = nn.Dropout(0.2,inplace=True)\n",
        "    fc = nn.Linear(in_features=1280,out_features=num_classes,bias=True)\n",
        "    classifier = nn.Sequential(dropout,fc)\n",
        "    pool = nn.AdaptiveAvgPool2d(1)\n",
        "    decoder_layers.append(pool)\n",
        "    decoder_layers.append(Flatten())\n",
        "    decoder_layers.append(classifier)\n",
        "\n",
        "    EncDec_dict['encoder'] = nn.Sequential(*encoder_layers)\n",
        "    EncDec_dict['decoders'] = [nn.Sequential(*decoder_layers)] # listed for a list of decoders\n",
        "\n",
        "    ## Creating a list of different Decoders\n",
        "    while decoder_copies > 1:\n",
        "        new_decoder = copy.deepcopy(nn.Sequential(*decoder_layers))\n",
        "        # new_decoder.apply(weight_noise)\n",
        "        EncDec_dict['decoders'].append(new_decoder)\n",
        "        decoder_copies -= 1\n",
        "\n",
        "    return EncDec_dict\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "############################# QUANTIZER #############################\n",
        "#####################################################################\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, codebook_size, commitment, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._num_vectors = codebook_size\n",
        "        self._size_vectors = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_vectors, self._size_vectors)\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(self._num_vectors))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(self._num_vectors, self._size_vectors))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        # inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._size_vectors)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
        "                     + torch.sum(self._embedding.weight ** 2, dim=1)\n",
        "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_vectors, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                    (self._ema_cluster_size + self._epsilon)\n",
        "                    / (n + self._num_vectors * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.contiguous(), encodings\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "############################### MODEL ###############################\n",
        "#####################################################################\n",
        "\n",
        "class SideFuncs(nn.Module):\n",
        "    def __init__(self, encoder, decoder, primary_loss, n_embed, decay=0.8, commitment=1., eps=1e-5,\n",
        "                 skip_quant=False, learning_rate=1e-3,training=True):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.primary_loss = primary_loss\n",
        "        self.commitment_w = commitment\n",
        "        dummy_input = torch.zeros((1, 3, 100, 100)) # Check number of channels the encoder outputs\n",
        "        self.quant_dim = encoder(dummy_input).shape[1]\n",
        "        self.quantizer = VectorQuantizerEMA(num_embeddings=self.quant_dim,\n",
        "                                            codebook_size=self.n_embed,  # size of the dictionary\n",
        "                                            commitment=1.0,\n",
        "                                            # the weight on the commitment loss (==1 cause we want control))\n",
        "                                            decay=self.decay)\n",
        "                                            # the exponential moving average decay, lower means the dictionary will change faster\n",
        "\n",
        "        self.skip_quant = skip_quant\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        z_e = self.encoder(x)\n",
        "        z_e = z_e.view((z_e.shape[0], z_e.shape[2], z_e.shape[3], z_e.shape[1]))\n",
        "        return z_e\n",
        "\n",
        "    def quantize(self,z_e):\n",
        "        if not self.skip_quant:\n",
        "            commit_loss, z_q, indices = self.quantizer(z_e)\n",
        "        else:\n",
        "            z_q, indices, commit_loss = z_e, None, 0\n",
        "        return z_q, indices, commit_loss\n",
        "\n",
        "\n",
        "    def decode(self, z):\n",
        "        predictions = []\n",
        "        for i, decoder in enumerate(self.decoder):\n",
        "            if i == 0:\n",
        "                predictions.append(decoder(z[0]))\n",
        "            else:\n",
        "                predictions.append(decoder(z[1]))\n",
        "        return predictions\n",
        "\n",
        "    def calculate_prime_loss(self, y_hat_list, y):\n",
        "        loss = 0\n",
        "        for y_hat in y_hat_list:\n",
        "            loss += self.primary_loss(y_hat, y)\n",
        "        return loss / len(y_hat_list)\n",
        "\n",
        "    def ensemble_calculator(self, preds_list):\n",
        "        return torch.mean(torch.stack(preds_list), axis=0)\n",
        "\n",
        "    def accuracy(self,y,y_pred,ensemble_y_pred):\n",
        "        ens_pred = torch.max(ensemble_y_pred.data, 1)[1]\n",
        "        batch_ens_corr = (ens_pred == y).sum()\n",
        "        predicted = []\n",
        "        batch_corr = []\n",
        "        for vec in range(len(y_pred)):\n",
        "            predicted.append(torch.max(y_pred[vec].data, 1)[1])\n",
        "            batch_corr.append((predicted[vec] == y).sum())\n",
        "        return batch_corr, batch_ens_corr\n",
        "\n",
        "\n",
        "\n",
        "class NeuraQuantModel(SideFuncs):\n",
        "    def __init__(self, encoder, decoder, primary_loss, n_embed=1024, decay=0.8, commitment=1., eps=1e-5,\n",
        "                 skip_quant=False, learning_rate=1e-3):\n",
        "        super().__init__(encoder, decoder, primary_loss,n_embed,decay, commitment, eps,\n",
        "                 skip_quant, learning_rate)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = nn.ModuleList(self.decoder)\n",
        "\n",
        "    def process_batch(self,batch):\n",
        "        x, y = batch\n",
        "        z_e = self.encoder(x)\n",
        "        z_e_for_quant = z_e.view((z_e.shape[0], z_e.shape[2], z_e.shape[3], z_e.shape[1]))\n",
        "        commit_loss, z_q, indices = self.quantizer(z_e_for_quant)\n",
        "        z_q = z_q.view((z_q.shape[0], z_q.shape[3], z_q.shape[1], z_q.shape[2]))\n",
        "        z_q = [z_e,z_q]\n",
        "        y_hat = self.decode(z_q)\n",
        "        ensemble_y_hat = self.ensemble_calculator(y_hat)\n",
        "        batch_acc, batch_acc_ensemble = self.accuracy(y, y_hat, ensemble_y_hat)\n",
        "        prime_loss = self.calculate_prime_loss(y_hat,y)\n",
        "        result_dict = {'loss': prime_loss + commit_loss, 'preds': y_hat, 'gts': y}\n",
        "        return result_dict, batch_acc, batch_acc_ensemble, y_hat\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "############################# TRAINING ##############################\n",
        "#####################################################################\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_correct = []\n",
        "epoch_acc = []\n",
        "ensemble_epoch_acc = []\n",
        "dec_acc = []\n",
        "np.random.seed(100)\n",
        "\n",
        "\n",
        "# Number of parameters\n",
        "def get_n_params(model):\n",
        "    pp = 0\n",
        "    for p in list(model.parameters()):\n",
        "        nn = 1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn * s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "\n",
        "def train(model,EncDec_dict,optimizer):\n",
        "    start_time = time.time()\n",
        "    val_acc = []\n",
        "    for epc in range(EPOCHS):\n",
        "        trn_corr = [0]*len(EncDec_dict['decoders'])\n",
        "        ensemble_corr = 0\n",
        "        losses = 0\n",
        "        for batch_num, (Train, Labels) in enumerate(trainloader):\n",
        "            batch_num += 1\n",
        "            batch = (Train.to(device), Labels.to(device))\n",
        "            result_dict, batch_acc, batch_acc_ensemble, _ = model.process_batch(batch)\n",
        "            loss = result_dict['loss']\n",
        "            losses += loss.item()\n",
        "            ensemble_corr += batch_acc_ensemble\n",
        "            for num in range(len(batch_acc)):\n",
        "                trn_corr[num] += batch_acc[num]\n",
        "\n",
        "    #        if grad_clip:\n",
        "    #           nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            # Update parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if batch_num % 100 == 0:\n",
        "                print(f'epoch: {epc+1:2}  batch: {batch_num:2} [{BATCH_SIZE * batch_num:6}/{len(trainset)}]  total loss: {loss.item():10.8f}  \\\n",
        "                time = [{(time.time() - start_time)/60}] minutes')\n",
        "\n",
        "        # scheduler.step()\n",
        "        ### Accuracy ###\n",
        "        loss = losses/batch_num\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        ensemble_epoch_acc.append((ensemble_corr.item()/(BATCH_SIZE*batch_num))*100)\n",
        "\n",
        "        for acc in range(len(trn_corr)):\n",
        "            dec_acc[acc].append((trn_corr[acc].item()/(BATCH_SIZE*batch_num))*100)\n",
        "\n",
        "        num_ens_correct=0\n",
        "        num_dec_correct = [0]*len(EncDec_dict['decoders'])\n",
        "        model.eval()\n",
        "        test_losses_val = 0\n",
        "        with torch.no_grad():\n",
        "            for b, (X_test, y_test) in enumerate(testloader):\n",
        "                # Apply the model\n",
        "                b += 1\n",
        "                batch = (X_test.to(device), y_test.to(device))\n",
        "                result_dict, test_batch_acc, test_batch_acc_ensemble, y_hat = model.process_batch(batch)\n",
        "                test_loss = result_dict['loss']\n",
        "                test_losses_val += test_loss.item()\n",
        "                num_ens_correct += test_batch_acc_ensemble\n",
        "                for lss in range(len(test_batch_acc)):\n",
        "                    num_dec_correct[lss] += test_batch_acc[lss]\n",
        "\n",
        "            test_losses.append(test_losses_val/b)\n",
        "        print(f'Train Ensemble Accuracy at epoch {epc + 1} is {100*ensemble_corr/len(trainset)}%')\n",
        "        print(f'Validation Ensemble Accuracy at epoch {epc+1} is {100*num_ens_correct/len(testset)}%')\n",
        "        val_acc.append(100*num_ens_correct/len(testset))\n",
        "\n",
        "\n",
        "        model.train()\n",
        "    duration = time.time() - start_time\n",
        "    print(f'Training took: {duration / 3600} hours')\n",
        "    return max(val_acc), y_hat\n",
        "\n",
        "\n",
        "num_parameters = []\n",
        "accuracy = []\n",
        "\n",
        "\n",
        "\n",
        "EncDec_dict = SplitNet(width=ARCH_WIDTH,\n",
        "                        pretrained=True,\n",
        "                        num_classes=NUM_CLASSES,\n",
        "                        stop_layer=PART_IDX,\n",
        "                        decoder_copies=8)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = NeuraQuantModel(encoder=EncDec_dict['encoder'],\n",
        "                        decoder=EncDec_dict['decoders'],\n",
        "                        primary_loss=criterion,\n",
        "                        n_embed=256,\n",
        "                        commitment=COMMITMENT_W)\n",
        "\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "for dec in range(len(EncDec_dict['decoders'])):\n",
        "    dec_acc.append([])\n",
        "\n",
        "\n",
        "def train_ens(ens,model,EncDec_dict,optimizer):\n",
        "    params = get_n_params(model)\n",
        "    num_parameters.append(params)\n",
        "    acc, y_hat = train(model,EncDec_dict,optimizer)\n",
        "    torch.save(y_hat,\"y_hat.pt\")\n",
        "    accuracy.append(acc)\n",
        "    print(f'Number of parameters for {ens + 1} users is : {params}')\n",
        "    print(f'Accuracy of {ens + 1} users is : {acc}')\n",
        "    return y_hat\n",
        "\n",
        "y_hat = train_ens(8,model,EncDec_dict,optimizer)\n",
        "\n",
        "\n",
        "print(num_parameters)\n",
        "print(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/model_ks_gpu.pth\"\n",
        "\n",
        "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.save(model.to(device).state_dict(), model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf6sZB_Y4xsT",
        "outputId": "fc14c591-f76d-49fe-ca00-80d4f3981eb8"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def get_val_acc(testloader):\n",
        "  model.eval()\n",
        "  user_single_acc = [0]*8\n",
        "  y_hats = [[],[],[],[],[],[],[],[]]\n",
        "  trues = []\n",
        "  with torch.no_grad():\n",
        "      for b, (X_test, y_test) in enumerate(testloader):\n",
        "          # Apply the model\n",
        "          b += 1\n",
        "          batch = (X_test.to(device), y_test.to(device))\n",
        "          _, test_batch_acc, _, y_hat = model.process_batch(batch)\n",
        "          trues.append(y_test)\n",
        "          for lss in range(len(test_batch_acc)):\n",
        "              user_single_acc[lss] += test_batch_acc[lss]\n",
        "              y_hats[lss].append(y_hat[lss])\n",
        "\n",
        "  user_single_acc = [100*acc / (b*BATCH_SIZE) for acc in user_single_acc]\n",
        "  return user_single_acc, y_hats, trues\n",
        "\n",
        "\n",
        "def calc_relative_acc(accuracies, sharpening):\n",
        "    relative_accuracies = []\n",
        "    sharped_accuracies = [acc**(sharpening) for acc in accuracies]\n",
        "    for i in range(len(sharped_accuracies)):\n",
        "      before_i = sharped_accuracies[:i]\n",
        "      after_i = sharped_accuracies[i + 1:]\n",
        "      rest = before_i + after_i\n",
        "\n",
        "      single_sharp_acc = sharped_accuracies[i] / sum(rest)\n",
        "      relative_accuracies.append(single_sharp_acc)\n",
        "    return relative_accuracies\n",
        "\n",
        "\n",
        "def get_weighted_val_acc(preds, true):\n",
        "  ens_pred = torch.max(preds.data, 2)[1]\n",
        "  num_corr = 0\n",
        "  for i in range(ens_pred.shape[0]):\n",
        "    num_corr += (ens_pred[i] == true[i]).sum()\n",
        "  val_acc = 100*num_corr/(len(testset)-16)\n",
        "  return val_acc\n"
      ],
      "metadata": {
        "id": "utYMv0rX5SaD"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_with_k(weight_factor, sharpening=8):\n",
        "    user_single_acc, y_hats, trues = get_val_acc(testloader)\n",
        "    relative_accuracies = calc_relative_acc(user_single_acc, sharpening)\n",
        "\n",
        "    first_user_acc = relative_accuracies[0]\n",
        "    rest_users_acc = relative_accuracies[1:]\n",
        "    first_user = y_hats[0]\n",
        "    rest_users = y_hats[1:]\n",
        "\n",
        "    alpha_1 = first_user_acc / (first_user_acc + weight_factor * sum(rest_users_acc))\n",
        "    alpha_rest = [(weight_factor * rest_user_acc) / (first_user_acc + weight_factor * sum(rest_users_acc)) for\n",
        "                  rest_user_acc in rest_users_acc]\n",
        "\n",
        "    result_list = [alpha_rest[0] * tensor for j, tensor in enumerate(rest_users[0]) if j < 156]\n",
        "    rest_weighted_users = result_list\n",
        "    # Calculate the dot product\n",
        "    summed_list = []\n",
        "    for i in range(1,len(rest_users) - 1):\n",
        "        result_list = [alpha_rest[i] * tensor for j, tensor in enumerate(rest_users[i]) if j < 156]\n",
        "\n",
        "        for tensor1, tensor2 in zip(rest_weighted_users, result_list):\n",
        "            summed_list.append(tensor1 + tensor2)\n",
        "        rest_weighted_users = summed_list\n",
        "        summed_list = []\n",
        "\n",
        "    weightning_preds = []\n",
        "    for tensor1, tensor2 in zip(first_user[:156], rest_weighted_users):\n",
        "        weightning_preds.append(alpha_1 * tensor1 + tensor2)\n",
        "\n",
        "    val_acc = get_weighted_val_acc(torch.stack(weightning_preds).to(device), torch.stack(trues[:156]).to(device))\n",
        "    print(val_acc)\n",
        "    return val_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "4oKRKhsf49WV"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqrt_8 = np.sqrt(8)\n",
        "weight_factors = [1/(8*sqrt_8),1/(7*sqrt_8),1/(6*sqrt_8),1/(5*sqrt_8),1/(4*sqrt_8),1/(3*sqrt_8),1/(2*sqrt_8),1/(sqrt_8),2/(sqrt_8),3/(sqrt_8),4/(sqrt_8),5/(sqrt_8),6/(sqrt_8)\n",
        "                  ,7/(sqrt_8),8/(sqrt_8),9/(sqrt_8)]\n",
        "\n",
        "accuracies = []\n",
        "for i in range(len(weight_factors)):\n",
        "    acc = validate_with_k(weight_factors[i])\n",
        "    accuracies.append(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgHbjAcR5DeC",
        "outputId": "3ad5c04c-f82e-4a6d-bb38-7ad006f43001"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(73.2873, device='cuda:0')\n",
            "tensor(73.4375, device='cuda:0')\n",
            "tensor(73.6278, device='cuda:0')\n",
            "tensor(73.8682, device='cuda:0')\n",
            "tensor(74.0885, device='cuda:0')\n",
            "tensor(74.5393, device='cuda:0')\n",
            "tensor(75.2204, device='cuda:0')\n",
            "tensor(75.8213, device='cuda:0')\n",
            "tensor(75.7212, device='cuda:0')\n",
            "tensor(75.5208, device='cuda:0')\n",
            "tensor(75.4908, device='cuda:0')\n",
            "tensor(75.4107, device='cuda:0')\n",
            "tensor(75.3305, device='cuda:0')\n",
            "tensor(75.3305, device='cuda:0')\n",
            "tensor(75.2704, device='cuda:0')\n",
            "tensor(75.2204, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZk4nPFvFfHZ",
        "outputId": "4fe65c70-0bdb-4e97-c61d-9c9f7093c21d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(73.2873, device='cuda:0'), tensor(73.4375, device='cuda:0'), tensor(73.6278, device='cuda:0'), tensor(73.8682, device='cuda:0'), tensor(74.0885, device='cuda:0'), tensor(74.5393, device='cuda:0'), tensor(75.2204, device='cuda:0'), tensor(75.8213, device='cuda:0'), tensor(75.7212, device='cuda:0'), tensor(75.5208, device='cuda:0'), tensor(75.4908, device='cuda:0'), tensor(75.4107, device='cuda:0'), tensor(75.3305, device='cuda:0'), tensor(75.3305, device='cuda:0'), tensor(75.2704, device='cuda:0'), tensor(75.2204, device='cuda:0')]\n"
          ]
        }
      ]
    }
  ]
}